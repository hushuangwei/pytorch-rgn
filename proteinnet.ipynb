{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "proteinnet.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVvONCEGXRhW"
      },
      "source": [
        "# Install conda and related\n",
        "\n",
        "https://towardsdatascience.com/conda-google-colab-75f7c867a522"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owg7GUJTVbZp",
        "outputId": "89d1870f-15f3-4484-8ca8-f20148421950"
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process is interrupted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlkxxJ30VvpE"
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.7 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQwFMNqWWJ90",
        "outputId": "aa52fded-3874-4ead-e857-4e025105ea97"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN0_xkYbWi_m"
      },
      "source": [
        "!conda install --channel conda-forge bcolz --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgujyeQFXBL7"
      },
      "source": [
        "# setting the environmental variables\n",
        "import sys\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.7/site-packages\"))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Y9IDVkZlm1"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW9JDGxoZrqQ",
        "outputId": "fed658ab-fa75-478f-ffde-e02ebaf1eb14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp11.tar.gz"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-14 16:05:25--  https://sharehost.hms.harvard.edu/sysbio/alquraishi/proteinnet/human_readable/casp11.tar.gz\n",
            "Resolving sharehost.hms.harvard.edu (sharehost.hms.harvard.edu)... 134.174.159.103\n",
            "Connecting to sharehost.hms.harvard.edu (sharehost.hms.harvard.edu)|134.174.159.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11603494538 (11G) [application/x-gzip]\n",
            "Saving to: ‘casp11.tar.gz’\n",
            "\n",
            "casp11.tar.gz       100%[===================>]  10.81G   105MB/s    in 1m 50s  \n",
            "\n",
            "2021-08-14 16:07:15 (101 MB/s) - ‘casp11.tar.gz’ saved [11603494538/11603494538]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNEzPXpaa33C",
        "outputId": "295d8855-dee9-41cf-8acd-8760cbe3f676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!tar -xzvf casp11.tar.gz"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "casp11/\n",
            "casp11/training_50\n",
            "casp11/training_95\n",
            "casp11/training_30\n",
            "casp11/testing\n",
            "casp11/training_90\n",
            "casp11/training_70\n",
            "casp11/validation\n",
            "casp11/training_100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82o2wvjhXmJn"
      },
      "source": [
        "# Run the original code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-hFtJgOUgNn"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import bcolz"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLZcDQyUgNr"
      },
      "source": [
        "pn_path = os.curdir + '/casp11/'\n",
        "data_path = os.curdir "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3peXdYuYUgNs",
        "outputId": "54199d83-19de-4865-f323-5eb46a9665cb"
      },
      "source": [
        "ids = []\n",
        "seqs = []\n",
        "evs = []\n",
        "coords = []\n",
        "masks = ['init', '/n']\n",
        "id_next, pri_next, ev_next, ter_next, msk_next = False, False, False, False, False\n",
        "with open(pn_path+'training_30') as fp:\n",
        "    for line in tqdm(iter(fp.readline, '')):\n",
        "        if id_next: ids.append(line[:-1])\n",
        "        elif pri_next: seqs.append(line[:-1])\n",
        "        elif ev_next: evs.append(np.genfromtxt(BytesIO(line.encode())))\n",
        "        elif ter_next: coords.append(np.genfromtxt(BytesIO(line.encode())))\n",
        "        elif msk_next: masks.append(line[:-1])\n",
        "        \n",
        "        if np.core.defchararray.find(line, \"[ID]\", end=5) != -1:\n",
        "            id_next = True\n",
        "            masks.pop()\n",
        "            masks.pop()\n",
        "            pri_next, ev_next, ter_next, msk_next = False, False, False, False\n",
        "        elif np.core.defchararray.find(line, \"[PRIMARY]\", end=10) != -1:\n",
        "            pri_next = True\n",
        "            ids.pop()\n",
        "            id_next, ev_next, ter_next, msk_next = False, False, False, False\n",
        "        elif np.core.defchararray.find(line, \"[EVOLUTIONARY]\", end=15) != -1:\n",
        "            ev_next = True\n",
        "            seqs.pop()\n",
        "            id_next, pri_next, ter_next, msk_next = False, False, False, False\n",
        "        elif np.core.defchararray.find(line, \"[TERTIARY]\", end=11) != -1:\n",
        "            ter_next = True\n",
        "            evs.pop()\n",
        "            id_next, pri_next, ev_next, msk_next = False, False, False, False\n",
        "        elif np.core.defchararray.find(line, \"[MASK]\", end=7) != -1:\n",
        "            msk_next = True\n",
        "            coords.pop()\n",
        "            id_next, pri_next, ev_next, ter_next = False, False, False, False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "608064it [23:44, 286.46it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fpcyblJUgNv"
      },
      "source": [
        "print('# IDs: {}'.format(len(ids)))\n",
        "print('# Seqs: {}'.format(len(seqs)))\n",
        "print('# PSSMs: {}'.format(len(evs)))\n",
        "print('# Coords: {}'.format(len(coords)))\n",
        "print('# Masks: {}'.format(len(masks[:-1]))) #-1 because of blank line at end of file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR6ADt4OUgNw"
      },
      "source": [
        "pssm = evs\n",
        "xyz = coords"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whmLnDquUgNw",
        "outputId": "b91d5531-b943-4cae-c59c-a019f90a832d"
      },
      "source": [
        "#loop through each evolutionary section\n",
        "for i in range(len(ids)):\n",
        "    #first store the id and sequence\n",
        "    id = ids[i]\n",
        "    seq = seqs[i]\n",
        "    \n",
        "    #next get the PSSM matrix for the sequence\n",
        "    sp = 21*i\n",
        "    ep = 21*(i+1)\n",
        "    psi = np.array(pssm[sp:ep])\n",
        "    pssmi = np.stack([p for p in psi], axis=1)\n",
        "    \n",
        "    #then get the coordinates\n",
        "    sx = 3*i\n",
        "    ex = 3*(i+1)\n",
        "    xi = np.array(xyz[sx:ex])\n",
        "    xyzi = np.stack([c for c in xi], axis=1)/100 #have to scale by 100 to match PDB\n",
        "    \n",
        "    #lastly convert the mask to indices\n",
        "    msk_idx = np.where(np.array(list(masks[i])) == '+')[0]\n",
        "    \n",
        "    #bracket id or get \"setting an array element with a sequence\"\n",
        "    zt = np.array([[id], seq, pssmi, xyzi, msk_idx])\n",
        "    \n",
        "    if i == 0:\n",
        "        bc = bcolz.carray([zt], rootdir=data_path+'validation.bc', mode='w', expectedlen=len(ids))\n",
        "        bc.flush()\n",
        "    else:\n",
        "        bc = bcolz.carray(rootdir=data_path+'validation.bc', mode='w')\n",
        "        bc.append([zt])\n",
        "        bc.flush()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbJhlF4TUgNx"
      },
      "source": [
        "#bcolz.open(rootdir=data_path+'testing.bc')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N68EXgBmc9QY"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYcBf5FIjZLE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import bcolz\n",
        "from shutil import copyfile\n",
        "from PIL import Image\n",
        "import Bio.PDB as bio\n",
        "import scipy\n",
        "import torch\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "residue_letter_codes = {'GLY': 'G','PRO': 'P','ALA': 'A','VAL': 'V','LEU': 'L',\n",
        "                        'ILE': 'I','MET': 'M','CYS': 'C','PHE': 'F','TYR': 'Y',\n",
        "                        'TRP': 'W','HIS': 'H','LYS': 'K','ARG': 'R','GLN': 'Q',\n",
        "                        'ASN': 'N','GLU': 'E','ASP': 'D','SER': 'S','THR': 'T'}\n",
        "\n",
        "aa2ix= {'G': 0,'P': 1,'A': 2,'V': 3,'L': 4,\n",
        "          'I': 5,'M': 6,'C': 7,'F': 8,'Y': 9,\n",
        "          'W': 10,'H': 11,'K': 12,'R': 13,'Q': 14,\n",
        "          'N': 15,'E': 16,'D': 17,'S': 18,'T': 19}\n",
        "\n",
        "#get sequences and corresponding chain ids\n",
        "def gather_sequence(pdb_id, seq_file):\n",
        "    seqs=[]\n",
        "    #chains=[]\n",
        "    #get line numbers of pdb_id\n",
        "    for ix, line in enumerate(seq_file):\n",
        "        pos = np.core.defchararray.find(line, pdb_id)\n",
        "        if pos > 0:\n",
        "            seqs.append(seq_file[ix+1][:-1]) #cut off newline character\n",
        "            #chains.append(line[pos+5]) #gives the chain letter from the line        \n",
        "        \n",
        "    return seqs\n",
        "\n",
        "def create_targets(pdb_file_path):\n",
        "    p = bio.PDBParser()\n",
        "    s = p.get_structure('X', pdb_file_path)\n",
        "    \n",
        "    #first = []\n",
        "    #last = []\n",
        "    #coords = []\n",
        "    #chains=[]\n",
        "    chain_coords=[]\n",
        "    \n",
        "    #randomly select one model from the pdb file\n",
        "    gen = s.get_models()\n",
        "    l = list(gen)\n",
        "    mod = l[np.random.randint(0, len(l))]\n",
        "    \n",
        "    #for model in s:\n",
        "    for chain in mod:\n",
        "        for ix,residue in enumerate(chain):\n",
        "            coords = []\n",
        "            #if ix == 0:\n",
        "            #    first.append(residue.get_id()[1])\n",
        "            if residue.get_id()[0] == ' ':\n",
        "                #l = residue.get_id()[1]\n",
        "                for atom in residue:\n",
        "                    if atom.get_name() == \"N\":\n",
        "                        n = atom.get_coord()\n",
        "                    elif atom.get_name() == \"CA\":\n",
        "                        ca = atom.get_coord()\n",
        "                    elif atom.get_name() == \"C\":\n",
        "                        cp = atom.get_coord()\n",
        "                try: #in some cases N is missing on the first residue, so we append zeros instead\n",
        "                    coords.append(np.stack([n,ca,cp], axis=0))\n",
        "                    del n, ca, cp\n",
        "                except:\n",
        "                    #first[-1] += 1 move past the first residue and ignore it\n",
        "                    pass\n",
        "                    #coords.append(np.zeros(3,3))\n",
        "        chain_coords.append(coords)\n",
        "        #coords = []\n",
        "        #chains.append(chain.get_id())\n",
        "        #last.append(l)\n",
        "    #final array is size 5x(no of residues)*3\n",
        "    return chain_coords #, chains, first, last\n",
        "\n",
        "def encode_sequence(sequence, onehot=True):\n",
        "    vec=[]\n",
        "    for chain in sequence:\n",
        "        for c in chain:\n",
        "            for aa, val in aa2ix.items():\n",
        "                if c == aa:\n",
        "                    vec.append(val)\n",
        "    if onehot:\n",
        "        encoding = to_categorical(vec, 20)\n",
        "        return np.uint8(encoding)\n",
        "    \n",
        "    return np.uint8(vec)\n",
        "\n",
        "def parse_pdb(pdb_file):\n",
        "    #pdb_file = 'pdb5l6t.ent' #np.random.choice(pdb_list)\n",
        "    p = bio.PDBParser()\n",
        "    s = p.get_structure('X', pdb_path+pdb_file)\n",
        "    \n",
        "    gen = s.get_models()\n",
        "    l = list(gen)\n",
        "    mod = l[np.random.randint(0, len(l))] #choose random model when more than 1 exists\n",
        "    \n",
        "    seq_strs = []\n",
        "    seq_locs = []\n",
        "    for chain in mod:\n",
        "        seq_str = ''\n",
        "        seq_loc = []\n",
        "        for residue in chain:\n",
        "            if residue.get_id()[0] == ' ':\n",
        "                letter_code = residue_letter_codes[residue.get_resname()]\n",
        "                seq_str += letter_code\n",
        "                for atom in residue:\n",
        "                    seq_loc.append(atom.get_full_id()[3][1])\n",
        "        seq_strs.append(seq_str)\n",
        "        seq_locs.append(np.unique(seq_loc))\n",
        "        \n",
        "    return seq_strs, seq_locs\n",
        "\n",
        "def align_indices(seq_strs, seq_locs, gt_seq, start_match_length=5):\n",
        "    fill_indices = []\n",
        "    for ix, pdb_seq in enumerate(seq_strs):\n",
        "        search_seq = gt_seq[ix]\n",
        "        pos = np.core.defchararray.find(search_seq, pdb_seq[:start_match_length])\n",
        "        if pos < 0:\n",
        "            raise ValueError('First 5 residues in pdb file have no match!')\n",
        "        locs = seq_locs[ix] + (pos - seq_locs[ix][0])\n",
        "        fill_indices.append(np.intersect1d(range(len(search_seq)), locs))\n",
        "    \n",
        "    return fill_indices\n",
        "\n",
        "def calc_dist(atom1_coord, atom2_coord):\n",
        "    return scipy.spatial.distance.euclidean(atom1_coord, atom2_coord)\n",
        "\n",
        "def gt_dihedral_angles(pdb_file_path):\n",
        "    p = bio.PDBParser()\n",
        "    s = p.get_structure('X', pdb_file_path)\n",
        "    calc_di = bio.vectors.calc_dihedral\n",
        "    calc_ba = bio.vectors.calc_angle\n",
        "    \n",
        "    #torsional angles\n",
        "    phi = []\n",
        "    psi = []\n",
        "    omega = []\n",
        "    #bond angles\n",
        "    ca_c_n = []\n",
        "    c_n_ca = []\n",
        "    n_ca_c = []\n",
        "    #bond_lengths\n",
        "    c_n = []\n",
        "    n_ca = []\n",
        "    ca_c = []\n",
        "    \n",
        "    for model in s:\n",
        "        for chain in model:\n",
        "            for ix, residue in enumerate(chain):\n",
        "                for atom in residue:\n",
        "                    if atom.get_name() == \"N\":\n",
        "                        n = atom.get_vector()\n",
        "                        n_coord = atom.get_coord()\n",
        "                        if ix != 0:\n",
        "                            psi.append(calc_di(np, cap, cp, n))\n",
        "                            ca_c_n.append(calc_ba(cap, cp, n))\n",
        "                            c_n.append(calc_dist(cp_coord, n_coord))\n",
        "                    if atom.get_name() == \"CA\":\n",
        "                        ca = atom.get_vector()\n",
        "                        ca_coord = atom.get_coord()\n",
        "                        if ix != 0:\n",
        "                            omega.append(calc_di(cap, cp, n, ca))\n",
        "                            c_n_ca.append(calc_ba(cp, n, ca))\n",
        "                            n_ca.append(calc_dist(n_coord, ca_coord))\n",
        "                    if atom.get_name() == \"C\":\n",
        "                        c = atom.get_vector()\n",
        "                        c_coord = atom.get_coord()\n",
        "                        if ix != 0:\n",
        "                            phi.append(calc_di(cp, n, ca, c))\n",
        "                            n_ca_c.append(calc_ba(n, ca, c))\n",
        "                            ca_c.append(calc_dist(ca_coord, c_coord))\n",
        "                #store previous vectors\n",
        "                np, cap, cp = n, ca, c\n",
        "                cp_coord = c_coord\n",
        "\n",
        "    torsional_angles = torch.stack([torch.tensor(psi), \n",
        "                                    torch.tensor(omega), \n",
        "                                    torch.tensor(phi)], dim=1)\n",
        "    \n",
        "    bond_angles = torch.stack([torch.tensor(ca_c_n), \n",
        "                               torch.tensor(c_n_ca), \n",
        "                               torch.tensor(n_ca_c)], dim=1)\n",
        "    \n",
        "    bond_lengths = torch.stack([torch.tensor(c_n), \n",
        "                                torch.tensor(n_ca), \n",
        "                                torch.tensor(ca_c)], dim=1)\n",
        "        \n",
        "    return torsional_angles, bond_angles, bond_lengths\n",
        "\n",
        "def subset(array_path, max_len, min_len=0, save_path=None):\n",
        "    \"\"\"Return a subset of a bcolz array based on max and min lengths of sequences\n",
        "    array_path: path to the bcolz array\n",
        "    max_len: maximum length of sequences to include in subset\n",
        "    min_len: Default 0, minimum length of sequences to include in subset\n",
        "    save_path: Default None, path to save subset array\n",
        "    \"\"\"\n",
        "    a = bcolz.carray(rootdir=array_path)\n",
        "    \n",
        "    shix = []\n",
        "    for ix in range(len(a)):\n",
        "        name, sequence, coords = a[ix]\n",
        "        length = len(sequence[0])\n",
        "        if (length >= min_len) and (length <= max_len):\n",
        "            shix.append(ix)\n",
        "    \n",
        "    if save_path:\n",
        "        subset = bcolz.carray(a[shix], rootdir=save_path, mode='w')\n",
        "        subset.flush()\n",
        "        return subset\n",
        "    \n",
        "    return a[shix]\n",
        "\n",
        "def save_array(fname, arr):\n",
        "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
        "    c.flush()\n",
        "\n",
        "def load_array(fname):\n",
        "    return bcolz.open(fname)[:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsHoUKYVjrxE"
      },
      "source": [
        "# RGN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMaOYhYCjuxE"
      },
      "source": [
        "import numpy as np\n",
        "import ipywidgets as ip\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import Counter as cs\n",
        "import sys\n",
        "import Bio.PDB as bio\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim\n",
        "import pdb\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQzS1AFbjy3w"
      },
      "source": [
        "from data import ProteinNet, sequence_collate\n",
        "from model import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7NF-FIYj3L9"
      },
      "source": [
        "from data import ProteinNet, sequence_collate\n",
        "from model import *\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaOmrs4sj-u6"
      },
      "source": [
        "## dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quM1542akC9C"
      },
      "source": [
        "#download data from github and run the proteinnet notebook first\n",
        "trn_dataset = ProteinNet(data_path+'train30.bc')\n",
        "val_dataset = ProteinNet(data_path+'validation.bc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHUiBwJskH7b"
      },
      "source": [
        "trn_data = DataLoader(trn_dataset, batch_size=32, shuffle=True, collate_fn=sequence_collate)\n",
        "val_data = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=sequence_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkS0cmk1kKu8"
      },
      "source": [
        "#there should be exactly 3 coordinates for each residue\n",
        "for i_batch, sample_batched in enumerate(trn_data):\n",
        "    vec = sample_batched['sequence']\n",
        "    print(i_batch, sample_batched['sequence'].size(),\n",
        "         sample_batched['coords'].size())\n",
        "    if i_batch == 3:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaIPmOLIkaL-"
      },
      "source": [
        "## RGN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3OVW-ickd-v"
      },
      "source": [
        "class RGN(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, linear_units=20, input_size=42):\n",
        "        super(RGN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.linear_units = linear_units\n",
        "        self.grads = {}\n",
        "        \n",
        "        self.lstm = nn.LSTM(self.input_size, hidden_size, num_layers, bidirectional=True)\n",
        "        \n",
        "        #initialize alphabet to random values between -pi and pi\n",
        "        u = torch.distributions.Uniform(-3.14, 3.14)\n",
        "        self.alphabet = nn.Parameter(u.rsample(torch.Size([linear_units, 3])))\n",
        "        self.linear = nn.Linear(hidden_size*2, linear_units)\n",
        "        \n",
        "        #set coordinates for first 3 atoms to identity matrix\n",
        "        self.A = torch.tensor([0., 0., 1.])\n",
        "        self.B = torch.tensor([0., 1., 0.])\n",
        "        self.C = torch.tensor([1., 0., 0.])\n",
        "\n",
        "        #bond length vectors C-N, N-CA, CA-C\n",
        "        self.avg_bond_lens = torch.tensor([1.329, 1.459, 1.525])\n",
        "        #bond angle vector, in radians, CA-C-N, C-N-CA, N-CA-C\n",
        "        self.avg_bond_angles = torch.tensor([2.034, 2.119, 1.937])\n",
        "\n",
        "    \n",
        "    def forward(self, sequences, lengths):\n",
        "        max_len = sequences.size(0)\n",
        "        batch_sz = sequences.size(1)\n",
        "        lengths = torch.tensor(lengths, dtype=torch.long, requires_grad=False)\n",
        "        order = [x for x,y in sorted(enumerate(lengths), key=lambda x: x[1], reverse=True)]\n",
        "        conv = zip(range(batch_sz), order) #for unorder after LSTM\n",
        "        \n",
        "        #add absolute position of residue to the input vector\n",
        "        abs_pos = torch.tensor(range(max_len), dtype=torch.float32).unsqueeze(1)\n",
        "        abs_pos = (abs_pos * torch.ones((1, batch_sz))).unsqueeze(2) #broadcasting\n",
        "        \n",
        "        h0 = Variable(torch.zeros((self.num_layers*2, batch_sz, self.hidden_size)))\n",
        "        c0 = Variable(torch.zeros((self.num_layers*2, batch_sz, self.hidden_size)))\n",
        "        \n",
        "        #input needs to be float32 and require grad\n",
        "        sequences = torch.tensor(sequences, dtype=torch.float32, requires_grad=True)\n",
        "        pad_seq = torch.cat([sequences, abs_pos], 2)\n",
        "    \n",
        "        packed = pack_padded_sequence(pad_seq[:, order], lengths[order], batch_first=False)\n",
        "        \n",
        "        lstm_out, _ = self.lstm(packed, (h0,c0))\n",
        "        unpacked, _ = pad_packed_sequence(lstm_out, batch_first=False, padding_value=0.0)\n",
        "        \n",
        "        #reorder back to original to match target\n",
        "        reorder = [x for x,y in sorted(conv, key=lambda x: x[1], reverse=False)]\n",
        "        unpacked = unpacked[:, reorder]\n",
        "\n",
        "        #for example, see https://bit.ly/2lXJC4m\n",
        "        softmax_out = F.softmax(self.linear(unpacked), dim=2)\n",
        "        sine = torch.matmul(softmax_out, torch.sin(self.alphabet))\n",
        "        cosine = torch.matmul(softmax_out, torch.cos(self.alphabet))\n",
        "        out = torch.atan2(sine, cosine)\n",
        "        \n",
        "        #create as many copies of first 3 coords as there are samples in the batch\n",
        "        broadcast = torch.ones((batch_sz, 3))\n",
        "        pred_coords = torch.stack([self.A*broadcast, self.B*broadcast, self.C*broadcast])\n",
        "        \n",
        "        for ix, triplet in enumerate(out[1:]):\n",
        "            pred_coords = geometric_unit(pred_coords, triplet, \n",
        "                                         self.avg_bond_angles, \n",
        "                                         self.avg_bond_lens)\n",
        "        #pred_coords.register_hook(self.save_grad('pc'))\n",
        "        \n",
        "            \n",
        "        #pdb.set_trace()\n",
        "        return pred_coords\n",
        "    \n",
        "    def save_grad(self, name):\n",
        "        def hook(grad): self.grads[name] = grad\n",
        "        return hook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL-6tDfTkiyN"
      },
      "source": [
        "#make sure output size and target sizes are the same\n",
        "for i_batch, sampled_batch in enumerate(trn_data):\n",
        "    inp_seq = sampled_batch['sequence']\n",
        "    inp_lens = sampled_batch['length']\n",
        "    rgn = RGN(20, 1, 20)\n",
        "    out = rgn(inp_seq, inp_lens)\n",
        "    print(i_batch, inp_seq.size(), sampled_batch['coords'].size(), out.size())\n",
        "    \n",
        "    if i_batch == 3:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89D5Vqhxkqrx"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNr3lLl3ktQp"
      },
      "source": [
        "#hyperparameters are directly from the paper's author\n",
        "rgn = RGN(800, 2, linear_units=60)\n",
        "#rgn.load_state_dict(torch.load(data_path+'models/rgn.pt')) #load pretrained model\n",
        "optimizer = torch.optim.Adam(rgn.parameters(), lr=1e-3)\n",
        "drmsd = dRMSD()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PH96Jukk3Hw"
      },
      "source": [
        "running_loss = 0.0\n",
        "\n",
        "for epoch in range(30):\n",
        "    last_batch = len(trn_data) - 1\n",
        "    for i, data in tqdm(enumerate(trn_data)):\n",
        "        names = data['name']\n",
        "        coords = data['coords']\n",
        "        mask = data['mask']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = rgn(data['sequence'], data['length'])\n",
        "\n",
        "        loss = drmsd(outputs, coords, mask)\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(rgn.parameters(), max_norm=50)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i != 0) and (i % last_batch == 0):\n",
        "            print('Epoch {}, Train Loss {}'.format(epoch, running_loss/i))\n",
        "            running_loss = 0.0\n",
        "            break\n",
        "            \n",
        "    last_batch = len(val_data) - 1\n",
        "    for i, data in tqdm(enumerate(val_data)):\n",
        "        names = data['name']\n",
        "        coords = data['coords']\n",
        "        mask = data['mask']\n",
        "        \n",
        "        outputs = rgn(data['sequence'], data['length'])\n",
        "        loss = drmsd(outputs, coords, mask)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i != 0) and (i % last_batch == 0):\n",
        "            print('Epoch {}, Val Loss {}'.format(epoch, running_loss/i))\n",
        "            running_loss = 0.0\n",
        "            \n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooWrdedgk8uR"
      },
      "source": [
        "torch.save(rgn.state_dict(), data_path+'models/rgn.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}